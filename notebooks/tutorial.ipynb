{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process NeuroPixels Electrophysiology data with DataJoint Elements\n",
    "\n",
    "This notebook will walk through processing NeuroPixels array electrophysiology\n",
    "data acquired with spikeGLX and processed with kilosort. While anyone can work\n",
    "through this notebook to process electrophysiology data through DataJoint's\n",
    "`element-array-ephys` pipeline, for a detailed tutorial about the fundamentals of\n",
    "DataJoint including table types, make functions, and querying, please see the [DataJoint Tutorial](https://github.com/datajoint/datajoint-tutorials).\n",
    "\n",
    "The DataJoint Python API and Element Array Electrophysiology offer a lot of features to support collaboration, automation, reproducibility, and visualizations.\n",
    "\n",
    "For more information on these topics, please visit our documentation: \n",
    " \n",
    "- [DataJoint Core](https://datajoint.com/docs/core/): General principles\n",
    "\n",
    "- DataJoint [Python](https://datajoint.com/docs/core/datajoint-python/) and\n",
    "  [MATLAB](https://datajoint.com/docs/core/datajoint-matlab/) APIs: in-depth reviews of\n",
    "  specifics\n",
    "\n",
    "- [DataJoint Element Array Ephys](https://datajoint.com/docs/elements/element-array-ephys/):\n",
    "  A modular pipeline for electrophysiology analysis\n",
    "\n",
    "\n",
    "Let's start by importing the packages necessary to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import datajoint as dj\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics:\n",
    "\n",
    "Any DataJoint workflow can be broken down into basic 3 parts:\n",
    "\n",
    "- `Insert`\n",
    "- `Populate` (or process)\n",
    "- `Query`\n",
    "\n",
    "In this demo we will:\n",
    "- `Insert` metadata about an animal subject, recording session, and \n",
    "  parameters related to processing electrophysiology data.\n",
    "- `Populate` tables with outputs of ephys recording data including LFPs, and spike\n",
    "  sorted waveforms and units.\n",
    "- `Query` the processed data from the database and plot waveform traces.\n",
    "\n",
    "Each of these topics will be explained thoroughly in this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow diagram\n",
    "\n",
    "This workflow is assembled from 4 DataJoint elements:\n",
    "+ [element-lab](https://github.com/datajoint/element-lab)\n",
    "+ [element-animal](https://github.com/datajoint/element-animal)\n",
    "+ [element-session](https://github.com/datajoint/element-session)\n",
    "+ [element-array-ephys](https://github.com/datajoint/element-array-ephys)\n",
    "\n",
    "Each element declares its own schema in the database. These schemas can be imported like\n",
    "any other Python package. This workflow is composed of schemas from each of the Elements\n",
    "above and correspond to a module within `workflow_array_ephys.pipeline`.\n",
    "\n",
    "The schema diagram is a good reference for understanding the order of the tables\n",
    "within the workflow, as well as the corresponding table type.\n",
    "Let's activate the elements and view the schema diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow_array_ephys.pipeline import subject, session, probe, ephys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(probe)\n",
    "    + dj.Diagram(ephys)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagram Breakdown\n",
    "\n",
    "While the diagram above seems complex at first, it becomes more clear when it's\n",
    "approached as a hierarchy of tables that **define the order** in which the\n",
    "workflow **expects to receive data** in each of its tables. \n",
    "\n",
    "- Tables with a green, or rectangular shape expect to receive data manually\n",
    "  using the `insert()` function. The tables higher up in the diagram such as\n",
    "  `subject.Subject()` should be the first to receive data. This ensures data\n",
    "  integrity by preventing orphaned data within DataJoint schemas. \n",
    "- Tables with a purple oval or red circle can be automatically filled with relevant data\n",
    "  by calling `populate()`. For example `ephys.EphysRecording` and its part-table\n",
    "  `ephys.EphysRecording.EphysFile` are both populated with\n",
    "  `ephys.EphysRecording.populate()`.\n",
    "- Tables connected by a solid line depend on attributes (entries) in the table\n",
    "  above it.\n",
    "\n",
    "#### Table Types\n",
    "\n",
    "There are 5 table types in DataJoint. Each of these appear in the diagram above.\n",
    "\n",
    "- **Manual table**: green box, manually inserted table, expect new entries daily, e.g. `Subject`, `ProbeInsertion`.  \n",
    "- **Lookup table**: gray box, pre inserted table, commonly used for general facts or parameters. e.g. `Strain`, `ClusteringMethod`, `ClusteringParamSet`.  \n",
    "- **Imported table**: blue oval, auto-processing table, the processing depends on the importing of external files. e.g. process of `Clustering` requires output files from kilosort2.  \n",
    "- **Computed table**: red circle, auto-processing table, the processing does not\n",
    "  depend on files external to the database.  \n",
    "- **Part table**: plain text, as an appendix to the master table, all the part\n",
    "  entries of a given master entry represent a intact set of the master entry.\n",
    "  e.g. `Unit` of a `CuratedClustering`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the workflow: Insert\n",
    "\n",
    "### Insert entries into manual tables\n",
    "\n",
    "To view details about a table's dependencies and attributes, use functions `.describe()`\n",
    "and `.heading`, respectively.\n",
    "\n",
    "Let's start with the first table in the schema diagram (the `subject` table) and view\n",
    "the table attributes we need to insert. There are two ways you can do this: *run each\n",
    "of the two cells below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table. These are particularly useful functions if you are new to\n",
    "DataJoint Elements and are unsure of the attributes required for each table. We will insert data into the\n",
    "`subject.Subject` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject='subject5',\n",
    "        subject_nickname='subject5',\n",
    "        subject_birth_date='2023-01-01',\n",
    "        sex='U',\n",
    "        subject_description='Example subject'\n",
    "    )\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table and see how the output varies between\n",
    "`.describe` and `.heading`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show the dependencies and attributes for the `session.Session` table.\n",
    "Notice that `describe` shows the dependencies of the table on upstream tables. The\n",
    "`Session` table depends on the upstream `Subject` table. \n",
    "\n",
    "Whereas `heading` lists all the attributes of the `Session` table, regardless of\n",
    "whether they are declared in an upstream table. \n",
    "\n",
    "Here we will demonstrate a very useful way of inserting data by assigning the dictionary\n",
    "to a variable `session_key`. This variable can be used to insert entries into tables that\n",
    "contain the `Session` table as one of its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject='subject5', \n",
    "                   session_datetime=datetime.datetime.now())\n",
    "\n",
    "session.Session.insert1(session_key)\n",
    "session.Session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SessionDirectory` table locates the relevant data files in a directory path\n",
    "relative to the root directory defined in your `dj.config[\"custom\"]`. More\n",
    "information about `dj.config` is provided at the end of this tutorial and is\n",
    "particularly useful for local deployments of this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.SessionDirectory.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(\n",
    "        session_key, \n",
    "        session_dir='subject5/session1'\n",
    "    )\n",
    ")\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the workflow diagram indicates, the tables in the `probe` schemas need to\n",
    "contain data before the tables in the `ephys` schema accept any data. Let's\n",
    "start by inserting into `probe.Probe`, a table containing metadata about a\n",
    "multielectrode probe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probe.Probe.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.Probe.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.Probe.insert1(\n",
    "    dict(probe=\"714000838\", \n",
    "         probe_type=\"neuropixels 1.0 - 3B\",\n",
    "         probe_comment=\"Example probe\")\n",
    ")  # this info could be achieve from neuropixels meta file.\n",
    "probe.Probe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probe metadata is used by the downstream `ProbeInsertion` table which we\n",
    "insert data into in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ephys.ProbeInsertion.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        insertion_number=1,\n",
    "        probe=\"714000838\",\n",
    "    )\n",
    ") # probe, subject, session_datetime needs to follow the restrictions of foreign keys.\n",
    "ephys.ProbeInsertion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate\n",
    "\n",
    "### Automatically populate tables\n",
    "\n",
    "`ephys.EphysRecording` is the first table in the pipeline that can be populated automatically.\n",
    "If a table contains a part table, this part table is also populated during the\n",
    "`populate()` call. `populate()` takes several arguments including the a session\n",
    "key. This key restricts `populate()` to performing the operation on the session\n",
    "of interest rather than all possible sessions which could be a time-intensive\n",
    "process for databases with lots of entries.\n",
    "\n",
    "Let's view the `ephys.EphysRecording` and its part table\n",
    "`ephys.EphysRecording.EphysFile` and populate both through a single `populate()`\n",
    "call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information was entered into each of these tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to spike sort the data with `kilosort`. An important step before\n",
    "processing is managing the parameters which will be used in that step. To do so, we will\n",
    "define the kilosort parameters in a dictionary and insert them into a DataJoint table\n",
    "`ClusteringParamSet`. This table keeps track of all combinations of your spike sorting\n",
    "parameters. You can choose which parameters are used during processing in a later step.\n",
    "\n",
    "Let's view the attributes and insert data into `ephys.ClusteringParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringParamSet.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert clustering task manually\n",
    "params_ks = {\n",
    "    \"fs\": 30000,\n",
    "    \"fshigh\": 150,\n",
    "    \"minfr_goodchannels\": 0.1,\n",
    "    \"Th\": [10, 4],\n",
    "    \"lam\": 10,\n",
    "    \"AUCsplit\": 0.9,\n",
    "    \"minFR\": 0.02,\n",
    "    \"momentum\": [20, 400],\n",
    "    \"sigmaMask\": 30,\n",
    "    \"ThPr\": 8,\n",
    "    \"spkTh\": -6,\n",
    "    \"reorder\": 1,\n",
    "    \"nskip\": 25,\n",
    "    \"GPU\": 1,\n",
    "    \"Nfilt\": 1024,\n",
    "    \"nfilt_factor\": 4,\n",
    "    \"ntbuff\": 64,\n",
    "    \"whiteningRange\": 32,\n",
    "    \"nSkipCov\": 25,\n",
    "    \"scaleproc\": 200,\n",
    "    \"nPCs\": 3,\n",
    "    \"useRAM\": 0,\n",
    "}\n",
    "ephys.ClusteringParamSet.insert_new_params(\n",
    "    clustering_method=\"kilosort2\",\n",
    "    paramset_idx=0,\n",
    "    params=params_ks,\n",
    "    paramset_desc=\"Spike sorting using Kilosort2\",\n",
    ")\n",
    "ephys.ClusteringParamSet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted kilosort parameters into the `ClusteringParamSet` table,\n",
    "we're almost ready to sort our data. DataJoint uses a `ClusteringTask` table to\n",
    "manage which `EphysRecording` and `ClusteringParamSet` should be used during processing. \n",
    "\n",
    "This table is important for defining several important aspects of\n",
    "downstream processing. Let's view the attributes to get a better understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ephys.ClusteringTask.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ClusteringTask` table contains two important attributes: \n",
    "+ `paramset_idx` \n",
    "+ `task_mode` \n",
    "\n",
    "The `paramset_idx` attribute is tracks\n",
    "your kilosort parameter sets. You can choose the parameter set using which \n",
    "you want spike sort ephys data. For example, `paramset_idx=0` may contain\n",
    "default parameters for kilosort processing whereas `paramset_idx=1` contains your custom parameters for sorting. This\n",
    "attribute tells the `Processing` table which set of parameters you are processing in a given `populate()`.\n",
    "\n",
    "The `task_mode` attribute can be set to either `load` or `trigger`. When set to `load`,\n",
    "running the processing step initiates a search for exisiting kilosort output files. When set to `trigger`, the\n",
    "processing step will run kilosort on the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        insertion_number=1,\n",
    "        paramset_idx=0,\n",
    "        task_mode='load', # load or trigger\n",
    "        clustering_output_dir=\"subject5/session1/probe_1/kilosort2-5_1\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we set the `task_mode` to `load`. Let's call populate on the `Clustering`\n",
    "table in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Clustering.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While spike sorting is completed in the above step, you can optionally curate\n",
    "the output of image processing using the `Curation` table. For this demo, we\n",
    "will simply use the results of the spike sorting output from the `Clustering` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Curation.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_key = (ephys.ClusteringTask & session_key).fetch1('KEY')\n",
    "ephys.Curation().create1_from_clustering_task(clustering_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `Curation` table receives an entry, we can populate the remaining\n",
    "tables in the workflow including `CuratedClustering`, `WaveformSet`, and `LFP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.CuratedClustering.populate(session_key, display_progress=True)\n",
    "ephys.LFP.populate(session_key, display_progress=True)\n",
    "ephys.WaveformSet.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've populated the tables in this workflow, there are one of\n",
    "several next steps. If you have an existing workflow for\n",
    "aligning waveforms to behavior data or other stimuli, you can easily\n",
    "invoke `element-event` or define your custom DataJoint tables to extend the\n",
    "pipeline.\n",
    "\n",
    "In this tutorial, we will do some exploratory analysis by fetching the data from the database and creating a few plots.\n",
    "\n",
    "## Query\n",
    "\n",
    "This section focuses on working with data that is already in the\n",
    "database. \n",
    "\n",
    "DataJoint queries allow you to view and import data from the database into a python\n",
    "variable using the `fetch()` method. \n",
    "\n",
    "There are several important features supported by `fetch()`:\n",
    "- By default, an empty `fetch()` imports a list of dictionaries containing all\n",
    "  attributes of all entries in the table that is queried.\n",
    "- **`fetch1()`**, on the other hand, imports a dictionary containing all attributes of\n",
    "  one of the entries in the table. By default, if a table has multiple entries,\n",
    "  `fetch1()` imports the first entry in the table.\n",
    "- Both `fetch()` and `fetch1()` accept table attributes as an argument to query\n",
    "  that particular attribute. For example `fetch1('fps')` will fetch the first\n",
    "  value of the `fps` attribute if it exists in the table.\n",
    "- Recommended best practice is to **restrict** queries by primary key attributes of the\n",
    "  table to ensure the accuracy of imported data.\n",
    "    - The most common restriction for entries in DataJoint tables is performed\n",
    "      using the `&` operator. For example to fetch all session start times belonging to\n",
    "      `subject1`, a possible query could be `subject1_sessions =\n",
    "      (session.Session & \"subject = 'subject1'\").fetch(\"session_datetime\")`.  \n",
    "- `fetch()` can also be used to obtain the primary keys of a table. To fetch the primary\n",
    "  keys of a table use `<table_name>.fetch(\"KEY\")` syntax.\n",
    "\n",
    "Let's walk through these concepts of querying by moving from simple to more\n",
    "complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_average = (ephys.LFP & \"insertion_number = '1'\").fetch1(\"lfp_mean\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query above, we fetch a single `lfp_mean` attribute from the `LFP` table.\n",
    "We also restrict the query to insertion number 1.\n",
    "\n",
    "Let's go ahead and plot the LFP mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lfp_average)\n",
    "plt.title(\"Average LFP Waveform for Insertion 1\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"microvolts (uV)\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint queries are a highly flexible tool to manipulate and visualize your data.\n",
    "After all, visualizing traces or generating rasters is likely just the start of\n",
    "your analysis workflow. This can also make the queries seem more complex at\n",
    "first. However, we'll walk through them slowly to simplify their content in this notebook. \n",
    "\n",
    "The examples below perform several operations using DataJoint queries:\n",
    "- Fetch the primary key attributes of all units that are in `insertion_number=1`.\n",
    "- Use **multiple restrictions** to fetch timestamps and create a raster plot.\n",
    "- Use a **join** operation and **multiple restrictions** to fetch a waveform\n",
    "  trace, along with unit data to create a single waveform plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_key = (ephys.ProbeInsertion & \"insertion_number = '1'\").fetch1(\"KEY\")\n",
    "units, unit_spiketimes = (ephys.CuratedClustering.Unit & insert_key & 'unit IN (\"6\",\"7\",\"9\",\"14\",\"15\",\"17\",\"19\")').fetch(\"unit\", \"spike_times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack(unit_spiketimes)\n",
    "y = np.hstack([np.full_like(s, u) for u, s in zip(units, unit_spiketimes)])\n",
    "plt.plot(x, y, \"|\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Unit\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_key = (ephys.CuratedClustering.Unit & insert_key & \"unit = '15'\").fetch1(\"KEY\")\n",
    "unit_data = (ephys.CuratedClustering.Unit * ephys.WaveformSet.PeakWaveform & unit_key).fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = (ephys.EphysRecording & insert_key).fetch1(\n",
    "    \"sampling_rate\"\n",
    ") / 1000  # in kHz\n",
    "plt.plot(\n",
    "    np.r_[: unit_data[\"peak_electrode_waveform\"].size] * 1 / sampling_rate,\n",
    "    unit_data[\"peak_electrode_waveform\"],\n",
    ")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(r\"Voltage ($\\mu$V)\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this tutorial notebook on your own data, please use the following steps:\n",
    "- Download the mysql-docker image for DataJoint and run the container according to the\n",
    "  instructions provide in the repository.\n",
    "- Create a fork of this repository to your GitHub account.\n",
    "- Clone the repository and open the files using your IDE.\n",
    "- Add a code cell immediately after the first code cell in the notebook - we will setup\n",
    "  the local connection using this cell. In this cell, type in the following code. \n",
    "\n",
    "```python\n",
    "import datajoint as dj\n",
    "dj.config[\"database.host\"] = \"localhost\"\n",
    "dj.config[\"database.user\"] = \"<your-username>\"\n",
    "dj.config[\"database.password\"] = \"<your-password>\"\n",
    "dj.config[\"custom\"] = {\"imaging_root_data_dir\": \"path/to/your/data/dir\",\n",
    "\"database_prefix\": \"<your-username_>\"}\n",
    "dj.config.save_local()\n",
    "dj.conn()\n",
    "```\n",
    "\n",
    "- Run this code cell and proceed with the rest of the notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff52d424e56dd643d8b2ec122f40a2e279e94970100b4e6430cb9025a65ba4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
